4.1、导入数据
在Sqoop中，“导入”概念指：从非大数据集群（RDBMS）向大数据集群（HDFS，HIVE，HBASE）中传输数据，叫做：导入，即使用import关键字。
4.1.1、RDBMS到HDFS
1) 确定Mysql服务开启正常
2) 在Mysql中新建一张表并插入一些数据
$ mysql -uroot -p000000
mysql> create database company;
mysql> create table company.staff(id int(4) primary key not null auto_increment, name varchar(255), sex varchar(255));
mysql> insert into company.staff(name, sex) values('Thomas', 'Male');
mysql> insert into company.staff(name, sex) values('Catalina', 'FeMale');

3) 导入数据
	（1）全部导入(mysql -> HDFS)
$ bin/sqoop import \
--connect jdbc:mysql://bigdata111:3306/company \
--username root \
--password 000000 \
--table staff \
--target-dir /user/company \
--delete-target-dir \
--num-mappers 1 \
--fields-terminated-by "\t"

	（2）查询导入(mysql查询 -> HDFS)
$ bin/sqoop import \
--connect jdbc:mysql://bigdata111:3306/company \
--username root \
--password 000000 \
--target-dir /user/company \
--delete-target-dir \
--num-mappers 1 \
--fields-terminated-by "\t" \
--query 'select name,sex from staff where id <=3 and $CONDITIONS;'
尖叫提示：must contain '$CONDITIONS' in WHERE clause.     注：CONDITIONS 翻译‘条件’
尖叫提示：如果query后使用的是双引号，则$CONDITIONS前必须加转移符，防止shell识别为自己的变量。
	（3）导入指定列
$ bin/sqoop import \
--connect jdbc:mysql://bigdata111:3306/company \
--username root \
--password 000000 \
--target-dir /user/company \
--delete-target-dir \
--num-mappers 1 \
--fields-terminated-by "\t" \
--columns id,sex \
--table staff
尖叫提示：columns中如果涉及到多列，用逗号分隔，分隔时不要添加空格
	（4）使用sqoop关键字筛选查询导入数据
$ bin/sqoop import \
--connect jdbc:mysql://bigdata111:3306/company \
--username root \
--password 000000 \
--target-dir /user/company \
--delete-target-dir \
--num-mappers 1 \
--fields-terminated-by "\t" \
--table staff \
--where "id=2"
尖叫提示：在Sqoop中可以使用sqoop import -D property.name=property.value这样的方式加入执行任务的参数，多个参数用空格隔开。
4.1.2、RDBMS到Hive
bin/sqoop import \
--connect jdbc:mysql://bigdata111:3306/company \
--username root \
--password 000000 \
--table staff \
--num-mappers 1 \
--hive-import \
--fields-terminated-by "\t" \
--hive-overwrite \
--hive-table staff_hive
尖叫提示：该过程分为两步，第一步将数据导入到HDFS，第二步将导入到HDFS的数据迁移到Hive仓库
尖叫提示：从MYSQL到Hive，本质时从MYSQL => HDFS => load To Hive
4.2、导出数据
在Sqoop中，“导出”概念指：从大数据集群（HDFS，HIVE，HBASE）向非大数据集群（RDBMS）中传输数据，叫做：导出，即使用export关键字。
4.2.1、HIVE/HDFS到RDBMS
创建aca表
create table abc(id int,name VARCHAR(5));

$ bin/sqoop export \
--connect jdbc:mysql://bigdata111:3306/Andy \
--username root \
--password 000000 \
--export-dir /user/hive/warehouse/staff_hive \
--table abc \
--num-mappers 1 \
--input-fields-terminated-by "\t"
尖叫提示：Mysql中如果表不存在，不会自动创建,自行根据表结构创建
思考：数据是覆盖还是追加 答案：追加
4.3、脚本打包
使用opt格式的文件打包sqoop命令，然后执行
1) 创建一个.opt文件
$ touch job_HDFS2RDBMS.opt

2) 编写sqoop脚本
$ vi ./job_HDFS2RDBMS.opt
#以下命令是从staff_hive中追加导入到mysql的aca表中

export
--connect
jdbc:mysql://bigdata111:3306/Andy
--username
root
--password
000000
--table
aca
--num-mappers
1
--export-dir
/user/hive/warehouse/staff_hive
--input-fields-terminated-by
"\t"

3) 执行该脚本
$ bin/sqoop --options-file job_HDFS2RDBMS.opt


